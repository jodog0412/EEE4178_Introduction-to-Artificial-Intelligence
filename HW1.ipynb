{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1V--YNCyZk8pH655EnDpwWuvNZZT6U343",
      "authorship_tag": "ABX9TyN1o96EnIUD5qsmlapz5vDG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jodog0412/EEE4178_Introduction-to-Artificial-Intelligence/blob/main/HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5bifRZNf4eA"
      },
      "outputs": [],
      "source": [
        "# coding: utf-8\n",
        "\n",
        "# 인공지능(딥러닝)개론 # Homework 1\n",
        "# 간단한 XOR Table을 학습하는 NN을 구성하는 문제입니다.\n",
        "# \n",
        "#  1-Layer, 2-Layer model을 각각 구성하여 XOR 결과를 비교합니다.\n",
        "#  1-Layer, 2-Layer의 model을 feedforward network와 Backpropagation을 이용하여 학습시킵니다.\n",
        "#  주어진 양식을 활용해 주시며, scale, 차원의 순서, hyper parameter등은 결과가 잘 나오는 방향으로 Tuning하셔도 무방합니다.\n",
        "#  Layer의 Activation 함수인 Tanh는 54~57번째 줄의 함수를 사용하시면 됩니다.\n",
        "#  결과 재현을 위해 Weight, bias 값을 저장하여 함께 첨부해 주시기 바랍니다.\n",
        "#  각 모델에서 loss 그래프와 testing step을 첨부하여 간단하게 자유 양식 결과 보고서(2~3장 내외)로 작성해 주세요.\n",
        "# \n",
        "# \n",
        "#  양식에서 활용하는 라이브러리 외에 추가로 import 하여 사용하실 수 없습니다.\n",
        "\n",
        "\n",
        "\n",
        "## 이 외에 추가 라이브러리 사용 금지\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Hyper parameters\n",
        "## 학습의 횟수와 Gradient update에 쓰이는 learning rate입니다.\n",
        "## 다른 값을 사용하여도 무방합니다.\n",
        "epochs = 10000\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "\n",
        "# Input data setting\n",
        "## XOR data \n",
        "## 입력 데이터들, XOR Table에 맞게 정의해놓았습니다.\n",
        "train_inp = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "train_out = np.array([0, 1, 1, 0])\n",
        "\n",
        "\n",
        "\n",
        "# Weight Setting\n",
        "## 학습에 사용되는 weight들의 초기값을 선언해 줍니다. random이 아닌 다른 값을 사용하여도 무방합니다.\n",
        "## 현재 weight변수는 2-layer 기준으로 설정되어있습니다.\n",
        "## 1-layer의 경우 W1과 b1을 문제에 맞게 바꿔 진행해주시면 됩니다. (방법은 자유)\n",
        "W1 = np.random.randn(2,2)\n",
        "W2 = np.random.randn(2,1)\n",
        "b1 = np.random.randn(1,2)\n",
        "b2 = np.random.randn(1,1)\n",
        "\n",
        "\n",
        "##-----------------------------------##\n",
        "##------- Activation Function -------##\n",
        "##-----------------------------------##\n",
        "def tanh(x):\n",
        "    numerator = np.exp(x) - np.exp(-x)\n",
        "    denominator = np.exp(x) + np.exp(-x)\n",
        "    return numerator/denominator\n",
        "\n",
        "\n",
        "# ----------------------------------- #\n",
        "# --------- Training Step ----------- #\n",
        "# ----------------------------------- #\n",
        "# 학습이 시작됩니다.\n",
        "# epoch 사이즈만큼 for 문을 돌며 학습됩니다.\n",
        "# 빈 칸을 채워 Weight과 bias를 학습(update)하는 신경망을 설계하세요.\n",
        "# 양식의 모든 내용을 무조건 따를 필요는 없습니다. 각자에게 편하게 수정하셔도 좋습니다. (변경한 경우 보고서에 작성 부탁드립니다.)\n",
        "\n",
        "errors = []\n",
        "for epoch in range(epochs):\n",
        "        \n",
        "    # 데이터 4가지 중 랜덤으로 하나 선택\n",
        "    for batch in range(4):\n",
        "        idx = random.randint(0,3)\n",
        "\n",
        "        # 입력 데이터 xin과 해당하는 정답 ans 불러오기\n",
        "        xin = train_inp[idx].reshape(1,2)\n",
        "        ans = train_out[idx]\n",
        "        \n",
        "        \n",
        "        # Layer에 맞는 Forward Network 구성\n",
        "        # HINT: 1-layer의 경우 net1만, 2-layer의 경우 net2까지 사용하시면 됩니다.\n",
        "        net1 = xin@(W1)+b1\n",
        "        y=tanh(net1)\n",
        "        net2 = y@(W2)+b2\n",
        "        z=tanh(net2)\n",
        "\n",
        "        # Mean Squared Error (MSE)로 loss 계산\n",
        "        loss = (ans-z)**2\n",
        "        \n",
        "        # delta matrix initialization(Zero 값이 아닌 다른 방법으로 이용하셔도 됩니다.)\n",
        "        delta_W1 = np.zeros((2,2))\n",
        "        delta_W2 = np.zeros((2,1))\n",
        "        delta_b1 = np.zeros((1,2))\n",
        "        delta_b2 = np.zeros((1,1))\n",
        "        \n",
        "        \n",
        "        # Backpropagation을 통한 Weight의 Gradient calculation(update)\n",
        "        y_grad=1-y**2\n",
        "        z_grad=1-z**2\n",
        "        delta_W2 = 2*(z-ans)*z_grad*(y.T)\n",
        "        delta_W1 = 2*(z-ans)*z_grad*np.sum(W2)*xin.T@y_grad\n",
        "        delta_b1 = 0\n",
        "        delta_b2 = 0\n",
        "\n",
        "    \n",
        "        # 각 weight의 update 반영\n",
        "        W1 = W1 - learning_rate * delta_W1\n",
        "        W2 = W2 - learning_rate * delta_W2\n",
        "        \n",
        "        b1 = b1 - learning_rate * delta_b1\n",
        "        b2 = b2 - learning_rate * delta_b2\n",
        "        \n",
        "        \n",
        "    ## 500번째 epoch마다 loss를 프린트 합니다.]\n",
        "    if epoch%500 == 0:\n",
        "        print(\"epoch[{}/{}] loss: {}\".format(epoch,epochs,loss[0].astype(np.float64)))\n",
        "        \n",
        "    ## plot을 위해 값 저장\n",
        "    errors.append(loss[0])\n",
        "\n",
        "\n",
        "\n",
        "## 학습이 끝난 후, loss를 확인합니다.\n",
        "loss =  np.array(errors)\n",
        "plt.plot(range(epochs),loss)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------------#\n",
        "#--------- Testing Step ------------#\n",
        "#-----------------------------------#\n",
        "\n",
        "for idx in range(4):\n",
        "    xin = train_inp[idx]\n",
        "    ans = train_out[idx]\n",
        "    \n",
        "    # 1-layer의 경우 net2는 사용하지 않아도 됩니다, 위에서 본인이 설정한 변수대로 진행하시면 됩니다.\n",
        "    net1 = xin@(W1)+b1\n",
        "    y=tanh(net1)\n",
        "    net2 = y@(W2)+b2\n",
        "    z=tanh(net2)\n",
        "\n",
        "    pred = z # ans와 가까울 수록 잘 학습된 것을 의미합니다.\n",
        "    \n",
        "    print(\"input: \", xin, \", answer: \", ans, \", pred: {}\".format(pred.astype(float)))\n",
        "    \n",
        "\n",
        "\n",
        "#-----------------------------------#\n",
        "#--------- Weight Saving -----------#\n",
        "#-----------------------------------#\n",
        "\n",
        "# weight, bias를 저장하는 부분입니다.\n",
        "# 학번에 자신의 학번으로 대체해 주세요.\n",
        "\n",
        "    #layer 1개인 경우\n",
        "#np.savetxt(\"학번_layer1_weight\",(W1, b1),fmt=\"%s\")\n",
        "\n",
        "    #layer 2개인 경우\n",
        "#np.savetxt(\"학번_layer2_weight\",(W1, W2, b1, b2),fmt=\"%s\")\n",
        "\n"
      ]
    }
  ]
}