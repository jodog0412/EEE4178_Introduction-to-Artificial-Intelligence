# -*- coding: utf-8 -*-
"""train_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IZKcSnr_e6amALvCHniE0Kq2ATGZJGGZ
"""

!cd /content/drive/MyDrive/Colab\ Notebooks/[EEE4178]\ 인공지능개론/HW3/dataset

!unzip /content/drive/MyDrive/Colab\ Notebooks/[EEE4178]\ 인공지능개론/HW3/dataset/ESC-50-master-16k.zip

from torch.utils.data import Dataset, TensorDataset, DataLoader
import torch
import torch.nn as nn
import torch.nn.functional as F
import librosa

import numpy as np
import pandas as pd

from glob import glob

# Device Configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load ESC-50 Data & Preprocess
metadata = pd.read_csv("./meta/esc50.csv")
wav_list = sorted(glob("./resample/*.wav"))

def spec_to_image(spec, eps=1e-6):
    mean = spec.mean()
    std = spec.std()
    spec_norm = (spec - mean) / (std + eps)
    spec_min, spec_max = spec_norm.min(), spec_norm.max()
    spec_scaled = 255 * (spec_norm - spec_min) / (spec_max - spec_min)
    spec_scaled = spec_scaled.astype(np.float32)
    return spec_scaled

class esc50dataset(Dataset):
    def __init__(self, wavlist, metadata):
        self.labels = np.array(metadata.target).astype(np.float32)
        self.audio = []
        for f in wavlist:
            wav, sr = librosa.load(f, sr=None)
            spec=librosa.feature.melspectrogram(wav, sr=sr, n_fft=1024, hop_length=640, n_mels=126) # [126,126]
            spec_db=librosa.power_to_db(spec,top_db=80)
            spec_image = np.expand_dims(spec_to_image(spec_db), axis=0)
            self.audio.append(spec_image.tolist())
        self.audio = np.array(self.audio)
    def __len__(self):
        return len(self.audio)
    def __getitem__(self, idx):
        return self.audio[idx], self.labels[idx]

# 30초 ~ 1분정도 소요
dataset = esc50dataset(wav_list, metadata)
features = dataset[:][0]
labels = dataset[:][1]

print(features.shape)
print(labels.shape)

######## train test split ######## Day4 실습 참고
## test data는 200개 이상 존재해야하며 train, val data와 중복되서는 안됩니다

train_size = 0.6
val_size = 0.5 

# make train set
split_id = int(len(features) * train_size) 
train_x, remain_x = features[:split_id], features[split_id:] 
train_y, remain_y = labels[:split_id], labels[split_id:]

# make val and test set
split_val_id = int(len(remain_x) * val_size) 
val_x, test_x = remain_x[:split_val_id], remain_x[split_val_id:]
val_y, test_y = remain_y[:split_val_id], remain_y[split_val_id:]

# define batch size # RuntimeError: CUDA out of memory 에러 출력 시 batch_size 낮추고 실행
batch_size = 16

# create tensor datasets
train_set = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))
valid_set = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))
test_set = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))

# create dataloaders
train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)
val_loader = DataLoader(valid_set, shuffle=True, batch_size=batch_size)
test_loader = DataLoader(test_set, shuffle=True, batch_size=batch_size)

print(len(train_loader)*batch_size)
print(len(val_loader)*batch_size)
print(len(test_loader)*batch_size)

audio, label = next(iter(train_loader))
print(len(test_y))
print(audio.size()) # [Batch, Channel, Height, Width]
print(label.size())

# Model Hyperparameter
##General 
num_classes = 50
num_epochs = 60
learning_rate = 0.004

##CNN
in_channel = 1
max_pool_kernel = 2

##RNN
sequence_length = 126
input_size = 32     #RNN input data의 차원
hidden_size = 96   # hidden state의 차원
num_layers = 1    # RNN의 은닉층 레이어 개수

######## Model ########
class CRNN(nn.Module):
  def __init__(self, input_size, hidden_size, num_layers, num_classes):
    super(CRNN, self).__init__()
    self.layer1=torch.nn.Sequential(
        nn.Conv2d(in_channels=in_channel,out_channels=16,kernel_size=5,stride=1,padding=2),
        nn.BatchNorm2d(16),
        nn.ReLU(),
        nn.MaxPool2d(max_pool_kernel) #16*63*63
        )
    self.layer2=torch.nn.Sequential(
        nn.Conv2d(in_channels=16,out_channels=32,kernel_size=5,stride=1,padding=3),
        nn.BatchNorm2d(32),
        nn.ReLU(),
        nn.MaxPool2d(max_pool_kernel) #32*32*32
    )
    self.hidden_size = hidden_size
    self.num_layers = num_layers
    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
    self.fc = nn.Linear(hidden_size, num_classes)

  def forward1(self,x):
      x = self.layer1(x)
      return x

  def forward2(self,x):
      x = self.layer2(x)
      return x
  
  def forward3(self,x):
      x=x.reshape(x.size(0),-1,input_size) #input: tensor [batch_size, seq_length, input_size]
      return x

  def forward4(self,x):
      out, _  = self.lstm(x) # output: tensor [batch_size, seq_length, hidden_size]
      return out

  def forward5(self,x):
      x = x[:,-1,:]
      x = self.fc(x)
      return x

  def forward(self, x):
    x = self.layer1(x)  #1*126*126->16*63*63
    x = self.layer2(x) #16*63*63->32*32*32
    x = F.relu(x)
    x = x.reshape(x.size(0),-1,input_size) #input: tensor [batch size, sequence length, input size]
    out, _  = self.lstm(x)  # output: tensor [batch_size, seq_length, hidden_size]
    out = self.fc(out[:,-1,:])
    # ou5 = self.sigmoid(out)
    return out

model = CRNN(input_size, hidden_size, num_layers, num_classes).to(device)

audio=audio.to(device,dtype=torch.float)
output1=model.forward1(audio)
print(f"[ Layer1 ] input : {audio.size()} --> output : {output1.size()}")
output2=model.forward2(output1)
print(f"[ Layer2 ] input : {output1.size()} --> output : {output2.size()}")
output3=model.forward3(output2)
print(f"[ Layer3 ] input : {output2.size()} --> output : {output3.size()}")
output4=model.forward4(output3)
print(f"[ Layer4 ] input : {output3.size()} --> output : {output4.size()}")
output5=model.forward5(output4)
print(f"[ Layer5 ] input : {output4.size()} --> output : {output5.size()}")
# _, pred = torch.max(output5.data, 1)
# print(label)
# print(pred)

######## Criterion & Optimizer ########
criterion = nn.CrossEntropyLoss().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

####### Train #######
model.train()
total_step = len(train_loader)
losses=[]
for epoch in range(num_epochs):
  correct = 0
  cnt = 0
  train_loss = 0
  for i, (audio, label) in enumerate(train_loader):
    
    audio = audio.to(device,dtype=torch.float)
    label = label.type(torch.LongTensor)
    label = label.to(device)

    # Forward
    output = model(audio)
    loss = criterion(output, label)

    _, pred = torch.max(output.data, 1)
    correct += (pred == label).sum().item()

    # Backward and optimize
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    train_loss += loss.item()
    
    if (i+1) == len(train_loader):
        losses.append(train_loss/len(train_loader))
        print("Epoch [{}/{}], Step[{}/{}], Loss:{:.4f}, Acc:{:.2f}%".format(epoch+1, num_epochs, i+1, total_step, train_loss/len(train_loader), correct/(len(train_loader)*batch_size) * 100))

import matplotlib.pyplot as plt
plt.plot(range(len(losses)),losses)

######## VALIDATION ########
with torch.no_grad():
  total_step = len(val_loader)
  correct = 0
  for i, (audio, label) in enumerate(val_loader):
    audio = audio.to(device,dtype=torch.float)
    label = label.type(torch.LongTensor)
    label = label.to(device)
    output = model(audio)
    _, pred = torch.max(output.data, 1)
    correct += (pred == label).sum().item()
    loss = criterion(output, label)
    train_loss += loss.item()
    losses.append(train_loss/total_step)
    print("Step[{}/{}], Loss:{:.4f}, Acc:{:.2f}%".format(i+1, total_step, train_loss/total_step, correct/(total_step*batch_size) * 100))
  print('Validation Accuracy of RNN model on the {} test images: {}%'.format(len(val_loader), 100 * correct / (len(val_loader)*batch_size)))

torch.save(model.state_dict(),"/content/drive/MyDrive/Colab Notebooks/[EEE4178] 인공지능개론/HW3/model/my_model.pth")   # model의 weight 정보 만을 저장

######## Test ########
test_model = CRNN(input_size, hidden_size, num_layers, num_classes).to(device)
test_model.load_state_dict(torch.load("/content/drive/MyDrive/Colab Notebooks/[EEE4178] 인공지능개론/HW3/model/my_model.pth"))
test_model.eval()

# metrics
test_acc = 0
with torch.no_grad():
    for audio, label in test_loader:
        audio, label = audio.to(device), label.to(device)
        audio = audio.to(device,dtype=torch.float)
        label = label.type(torch.LongTensor)
        label = label.to(device)

        # forward pass
        out = test_model(audio)

        # acc
        _, pred = torch.max(out, 1)
        test_acc += (pred==label).sum()
        
    print(f'Accuracy: {test_acc.cpu().numpy()/len(test_set) * 100}%')

